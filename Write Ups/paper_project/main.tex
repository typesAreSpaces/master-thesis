%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.


\documentclass[sigconf,authordraft]{acmart}

\usepackage{algpseudocode}
\usepackage{algorithm}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2020}
\acmYear{2020}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference{UNM 16th Annual Computer Science Student Conference}
{March 25, 2020}{Albuquerque, NM}
% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% June 03--05, 2018, Woodstock, NY}
\acmBooktitle{UNM 16th Annual Computer Science Student Conference,
  March 25th, 2020, Albuquerque, NM}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Interpolant generation for EUF using Ground Horn Clauses with Explanations}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Jos\'e Abel Castellanos Joo}
\email{jose.castellanosjoo@cs.unm.edu}
\author{Deepak Kapur}
\email{kapur@cs.unm.edu}
\affiliation{%
  \institution{University of New Mexico}
  \streetaddress{What's the address?}
  \city{Albuquerque}
  \state{New Mexico}
  \postcode{87106}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Castellanos and Kapur}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  TODO:
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010148.10010149.10010157</concept_id>
<concept_desc>Computing methodologies~Equation and inequality solving algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003790.10002990</concept_id>
<concept_desc>Theory of computation~Logic and verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Equation and inequality solving algorithms}
\ccsdesc[300]{Theory of computation~Logic and verification}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Craig's interpolant, free theory, congruence closure algorithms}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}

Interpolation algorithms for the theory of equality with uninterpreted functions are relevant
as the core part of program analysis and verification algorithms. Many useful technique in software
engineering like bounded/unbounded model checking and invariant generation benefit directly from
this technique. TODO. (There might be no more space to write anything else here.)

The contributions of this paper are the following:
\begin{itemize}
\item Performance improvement of the conditional replacement step in Kapur's algorithm
  for the interpolation generation of the conjunction of equalities in the theory of equality with
  uninterpreted functions.
\item An efficient algorithm implementing the Explanation operation
  for testing satisfiability of grounded Horn clauses.
\end{itemize}

\section{Background}

\subsection{Craig's Interpolant}

Let $\alpha, \beta, \gamma$ be logical formulas in a given theory. If
$\vdash \alpha \rightarrow \beta$, we say that $\gamma$ is an
interpolant for the pair $(\alpha, \beta)$ if the following conditions
are met:

\begin{itemize}
\item $\vdash \alpha \rightarrow \gamma$
\item $\vdash \gamma \rightarrow \beta$
\item Every non-logical symbol in $\gamma$ occurs both in $\alpha$ and
  $\beta$.
\end{itemize}

The \emph{interpolantion problem} can be stated naturally as follows: given two
logical formulas $\alpha, \beta$ such that $\vdash \alpha \rightarrow \beta$, find
the interpolant for the pair $(\alpha, \beta)$.

In his celebrated work \cite{10.2307/2963594}, Craig proved that for every pair
$(\alpha, \beta)$ of first order formulas such that
$\vdash \alpha \rightarrow \beta$, an interpolation formula exists.

Usually, we see the interpolation problem defined differently in the literature, where
we consider $\beta^{'}$ to be $\neg \beta$ and the problem requires that the pair $(\alpha, \beta^{'})$
is mutually contradictory (unsatisfiable). This definition was popularized by McMillan
\cite{10.1007/978-3-540-24730-2_2}. This shift of attention explains partially the further
development in interpolation generation algorithms since many of these relied on
SMT solvers that provided refutation proofs in order to (re)construct interpolants
for different theories (and their combination) \cite{10.1007/978-3-642-02959-2_17,
  10.1007/978-3-642-36742-7_9, mcmillan2011interpolants}.

Extended definitions are given to the interpolation problem when dealing with specific
theories \cite{10.1007/11532231_26} in a way that interpreted function can be also part of
the interpolant. The latter is justify since otherwise, many interpolation formulas might
not exists in different theories (for example, lisp programs).

We also see different approaches to interpolation generation for particular
theories that exploits aspects of the underlying theory \cite{}.

\subsection{Equality with Uninterpreted Functions}

The language of \emph{Equality with Uninterpreted Functions} (EUF) consists of the
equality symbol as the unique predicate symbol, and a countable number of functional
symbols (including constants of 0-arity). Its theory consists only of the axioms
defining the equality symbol to be an equivalence relation:

\begin{itemize}
\item \emph{Reflexivity} $\forall x . x = x$
\item \emph{Symmetry} $\forall x, y . x = y \rightarrow y = x$
\item \emph{Transitivity} $\forall x, y, z . (x = y \land y = z) \rightarrow x = z$
\item \emph{Congruence} $\forall \text{ n-arity function symbol}, n > 0 . \forall x_1,
  \dots, x_n, y_1, \dots, y_n . (x_1 = y_1 \land \dots \land x_n = y_n) \rightarrow
  f(x_1, \dots, x_n) = f(y_1, \dots, y_n)$
\end{itemize}

We notice that the Congruence axiom is not an first-order axiom but rather an axiom
scheme \cite{10.1145/322186.322198}. No other axiom is added. Because of this,
the EUF theory is also called the \emph{empty theory}.

\subsection{Kapur's Algorithm for EUF Theory}

Kapur's interpolation algorithm for the EUF theory uses quantifier-elimination
techniques to remove symbols in the first formula of an interpolation problem instance
that are not common with the second formula of the latter. 
Hence, the input for this algorithm is a conjunction of equalities in the
EUF theory and a set of symbols to eliminate, also unknown as uncommon symbols.

The steps/ideas in Kapur's algorithm for interpolant generation for the EUF theory
are the following:

\begin{itemize}
\item \textbf{Elimination of uncommon terms using Congruence Closure}. This step builds an equivalence
  relation using the input of the algorithm such that the representatives are common terms
  whenever possible. Let \emph{answer} be a variable denoting the conjunction of all the input formulas
  which uncommon subterms are replaced by their representatives.
  If all the representatives in the equivalence relation are common terms, return \emph{answer}
  as the interpolant. Otherwise, continue with the following step.
\item \textbf{Horn clause generation by exposure}. This step uses Ackermann's reduction \cite{10.5555/1391237}
  to produce Horn clauses to eliminate uncommon terms identifying two cases:
  \begin{itemize}
  \item The term is uncommon because the function symbol is uncommon.
  \item The term is uncommon because at least one of its arguments is
    an uncommon term.
  \end{itemize}
  Conjunct to \emph{answer} the conjunction of all these Horn Clauses.
  If all the Horn Clauses above are common, return \emph{answer} as the interpolant.
  Otherwise, continue with the following step.
\item \textbf{Conditional elimination}. Since some of the Horn clauses produced by
  the previous step are not common, we identify the Horn clauses that have
  \emph{common antecedents} and head equation with at least one uncommon term. For each
  of these Horn clauses, replace the uncommon term in its head by the common term in its head
  in the rest of the Horn Clauses, and include its antecedent to the antecedent of such Horn clauses.
  We can see that this step reduces the number of uncommon terms in the equalities of
  the Horn Clauses. We repeat this step until it cannot be performed. At the end, we take only the
  set of Horn Clauses that have common antecedents and have one uncommon term in its head. The call these
  Horn clauses \emph{useful Horn clauses}. Continue with the next step.
\item \textbf{Conditional replacement}. Using the \emph{useful Horn clauses} generated in the previous
  step, update \emph{answer} to be the formula resulting after \emph{conditionally replacing}
  uncommon terms in each equation of \emph{answer} by an appropriate common term in the head of
  a \emph{useful Horn clauses}.
  To be more precise, let $\bigwedge_i a_i = b_i \rightarrow u \mapsto c$ be a useful Horn clause,
  where the antecedent is a conjunction of common grounded equations, $u$ is an uncommon term,
  and $c$ is a common term.
  Then for every instance of $u$ in each equation of \emph{answer}, conditionally replace $u$
  by $c$ under $\bigwedge_i a_i = b_i$.
  We notice that equations in \emph{answer}  of the previous step will become Horn Clauses with
  less uncommon terms. For completeness, we perform these replacements zero or more times
  (up to the maximal number of instances per equation) in order to leave space for other
  \emph{useful Horn Clauses} to replace the uncommon term in their heads as well.
  Remove all the literals in the current \emph{answer} that contain uncommon terms and return this
  as the interpolant.
\end{itemize}

If the user is not interested in an explicit interpolant, we can present the \emph{usable Horn
  clauses} in a proper order such that the replacements can be done without exponentially increasing
the size of the interpolant. This representation is useful because it provides a more compact
representation of the interpolant that the user might be able quicker to obtain.
Additionally, the user might be just interested in a particular subformula of the interpolant, so the
latter representation offers feature.

This algorithm allows a flexible implementation which can lead several optimizations
based on the nature and applications of the interpolants. TODO.

\subsection{Ground Horn Clauses unsatisfiability testing}

In \cite{GALLIER1987233} it was proposed an algorithm for testing the unsatisfiability
of ground Horn clauses with equality. The main idea was to interleave two algorithms: \emph{implicational propagation}
(propositional satisfiability of Horn clauses) that updates the truth value of equations
in the antecedent of the input Horn clauses \cite{DOWLING1984267}; and \emph{equational propagation} (congruence closure
for grounded equations) to update the state of a Union-Find data structure \cite{10.1145/364099.364331}
that keeps the minimal equivalence relation defined by grounded equations in the input Horn clauses.

The author in \cite{GALLIER1987233} defined two variations of his algorithms by adapting
the Congruence Closure algorithms in \cite{10.1145/322217.322228, 10.1145/322186.322198}.
Additionally, modifications in the data structures used by the original algorithms were needed
to make the interleaving mechanism more efficient.

\subsection{Congruence Closure with Explanations}

In \cite{10.1007/978-3-540-32033-3_33}, the authors introduced a Union-Find
data structure that supports the Explanation operation. This operation receives
as input an equation between constants. If the input equation is a consequence
of the current equivalence relation defined in the Union Find data structure, the Explanation
operation returns the minimal sequence of equations used to build such equivalence relation,
otherwise it returns `Not provable`. A proper implementation of this algorithm extends the traditional
Union-Find data structure with a \emph{proof-forest}, which consists of an additional
representation of the underlying equivalence relation that does not compress
paths whenever a call to the Find operation is made. For efficient reasons, the Find operation
uses the path compression and weighted union.

The main observation in \cite{10.1007/978-3-540-32033-3_33} is that, in order to
recover an explanation between two terms, by traversing the path between the two nodes
in the proof tree, the last edge in the path guarantees to be part of the explanation.
Intuitively, this follows because only the last Union operation was responsible of merging
the two classes into one. Hence, we can recursively recover the rest of the
explanation by recursively traversing the subpaths found.

Additionally, the authors in \cite{10.1007/978-3-540-32033-3_33} extended the
Congruence Closure algorithm \cite{10.1007/978-3-540-39813-4_5} using the above
data structure to provide Explanations for EUF theory. The congruence closure algorithm
is a simplification of the congruence closure algorithm in \cite{10.1145/322217.322228}.
The latter combines the traditional \emph{pending} and \emph{combine} list into one
single list, hence removing the initial \emph{combination} loop in the algorithm in
\cite{10.1145/322217.322228}.

\section{Main contribution} \label{aha}

First, we explain a high level ideal on how we improve the \emph{conditional elimination}
step in Kapur's algorithm. We notice that this step \emph{propagates equationally} the
head equations of grounded Horn clauses with common antecedents. Initially we employ the
unsatisfiability algorithm for Horn clauses to achieve such propagation. However, the original
algorithm will not be enough because it will only propagate the head equation when all the
antecedents have truth value equal to true. To fix that problem, we modify two steps in Gallier's
algorithms:

\begin{itemize}
\item When we build the data structure \emph{numargs} that keeps track of the number of unproven
  equations in the antecedent of each Horn clause, we change this number by the number
  of unproven uncommon equations in the antecedent of each Horn clause. This will be useful
  because we only introduce head equations intro the queue data structure in Gallier's algorithm
  when all the antecedents are true. With this modification, our algorithm introduces head equations
  when all the antecedent equations are common. Additionally the algorithm can still update
  correctly the truth value of common equations, but these are not relevant for our propagation
  purposes.
\item To guarantee that \emph{numargs} keeps the right number of uncommon equations yet to
  be proven, we also modify the update mechanism for \emph{numargs} in the main while loop of the algorithm.
  The original algorithm reduces by one the corresponding entry in \emph{numargs}
  whenever a recently popped element from the queue matches the antecedent of a Horn clause. We only
  decrease this value if such popped equation is uncommon. This prevents the algorithm from accidentally
  reducing the number of uncommon equations yet to be proven, which can cause that we propagate the
  uncommon head equation when the antecedent of a Horn clause only consists of common equations.
\end{itemize}

At the end of this algorithm we can identify \emph{usable Horn clauses} by checking the Horn clauses
with \emph{numargs} entries equal to 0. Nonetheless, these Horn clauses are not the
desired \emph{usable Horn clauses} because the unsatisfiability testing algorithm
did not update the antecedents of the Horn clauses. The main difficulty to design a data structure
for the latter to work inside the unsatisfiability testing algorithm was the queue data structure
only adds grounded equation whenever the truth value of the literal changes to true, which happens
during \emph{equational propagation} or during the \emph{implicational propagation} steps.
For the \emph{implicational propagation} the task is easy because we can know the clause
where the just new proven ground equation comes, but it cannot be the same situation
for the \emph{equational propagation} since this step relies on the Congruence Closure.

To remedy this issue, we equip our Congruence Closure algorithm with the Explanation operator, so
we can recover the grounded equations needed to entail any particular grounded equation. Additionally,
this will require a data structure to maintain the Horn clauses for each grounded equation that
it is the head equation of. With the latter we can recover the Horn Clauses where each grounded
equation came from to update the antecedents and obtain \emph{usable Horn clauses}.

\subsection{New optimized conditional elimination step in Kapur's algorithm}

The algorithm appears below in pseudo code notation:

\begin{algorithm}
  \caption{Modified Unsatisfiability Testing for Ground Horn Clauses}
  \begin{algorithmic}[1]

    \Procedure {satisfiable}{var H : Hornclause; var queue, combine: queuetype; var GT(H) : Graph; var consistent : boolean}
    \While {queue not empty and consistent}
    \State node := pop(queue);
    \For {clause1 in H[node].clauselist}
    \If {$\neg$ clause1.isCommon()}
    \State {numargs[clause1] := numargs[clause1] - 1}
    \EndIf
    \If {numargs[clause1] = 0}
    \State nextnode := poslitlist[clause1];
    \If {$\neg$ H[nextnode].val}
    \If {nextnode $\neq \bot$ }
    \State {queue := push(nextnode, queue);}
    \State {H[nextnode].val := true;}
    \State {u := left(H[nextnode].atom); v := right(H[nextnode].atom);}
    \If {FIND(R, u) $\neq$ FIND(R, v)}
    \State {combine := push((u, v), combine);}
    \EndIf
    \Else
    \State {consistent := false;}
    \EndIf
    \EndIf
    \EndIf
    \EndFor
    \If {queue is empty and consistent}
    \State {closure(combine, queue, R);}
    \EndIf
    \EndWhile
    \EndProcedure
    \Statex
    \Procedure {closure}{var combine, queue : queuetype; var R : partition}
    \While {combine is not empty}
    \State (u, v) = pop(combine)
    \State MERGE(R, u, v, queue)
    \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Modified Congruence Closure with Explanation Algorithms}
  \begin{algorithmic}[2]
    \Procedure {Merge}{R : partition, u, v : node; queue, combine : queuetype}
    \If u and v are constants a and b
    \State add a = b to Pending; Propagate();
    \Else \Comment u=v is of the form f(a1, a2)=a
    \If Lookup(Representative(a1), Representative(a2)) is some f(b1, b2)=b
    \State add (f(a1, a2)=a, f(b1, b2) = b) to Pending; Propagate();
    \Else
    \State set Lookup(Representative(a1), Representative(a2)) to f(a1, a2)=a;
    \State add f(a1, a2)=a to UseList(Representative(a1)) and to UseList(Representative(a2));
    \EndIf
    \EndIf
    \EndProcedure
    \Statex
    \Procedure {Propagate} {\text{ }}
    \While Pending is non-empty
    \State Remove E of the form a=b or (f(a1, a2) = a, f(b1, b2) = b) from Pending
    \If $Representative(a) \neq Representative(b)$ and w.l.o.g. $|ClassList(Representative(a))| \leq |ClassList(Representative(b))|$
    \State oldReprA := Representative(a);
    \State Insert edge $a \rightarrow b$ labelled with E into the proof forest;
    \For each c in ClassList(oldReprA)
    \State set Representative(c) to Representative(b)
    \State move c from ClassList(oldReprA) to ClassList(Representative(b))
    \For each pointer L in ClassList(u)
    \If H[L].val = false
    \State set the field H[L].lclass or H[L].rclass pointed to by p to Representative(b)
    \If H[L].lclass = H[L].rclass
    \State queue := push(L, queue);
    \State H[L].val := true
    \EndIf
    \EndIf
    \EndFor
    \EndFor
    \For each f(c1, c2) = c in UseList(oldReprA)
    \If Lookup(Representative(c1), Representative(c2)) is some f(d1, d2) = d
    \State add (f(c1, c2) = c, f(d1, d2) = d) to Pending;
    \State remove f(c1, c2) = c from UseList(oldReprA);
    \Else
    \State set Lookup(Representative(c1), Representative(c2)) to f(c1, c2) = c;
    \State move f(c1, c2) = c from UseList(oldReprA) to UseList(Representative(b));
    \EndIf
    \EndFor 
    \EndIf
    \EndWhile
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Ground Horn Clauses with Explanations}

We notice that, by removing our changes to the unsatisfiability testing
for grounded Horn clauses regarding uncommon symbols, we effectively combine
the congruence closure with explanations to the original unsatisfiability
testing algorithm. With the latter, we can query the membership of a Horn
clauses in a given user-defined theory and additionally obtain a proof of
the latter. This approach works by introducing the antecedent equations of
a grounded Horn clause as part of the user-defined theory in order to prove
its head equation. By the Deduction Theorem \cite{10.5555/1642730}, we can
recover a proof of the original queried Horn clause by removing the antecedent
equations appearing the proof given by the Explain operation.

\section{Conclusions}
In this paper we have presented a new optimized step for conditional elimination
in Kapur's algorithm for EUF interpolant generation. As a by-product, we discover
an efficient algorithm to implement the Explain operator in grounded Horn clauses.
To our knowledge, we are the first authors to contribute with such operation for
grounded Horn clauses in particular. However, we can see that also an Explain
operator can be encoded using the proof-producing capabilities of SMT solvers
\cite{10.1007/978-3-540-78800-3_24, 10.5555/2032305.2032319, 10.1007/978-3-642-02959-2_12, articleProofsInSMT, inproceedingsproofsrefutationsz3}. Nonetheless, the latter
requires a refutational proof, our method might be able to outperform these
approaches.
For future work we plan to implement this algorithm to incorporate it into an
implementation of Kapur's algorithm and compare the proof-producing feature
with state of the art SMT solvers.

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
  To Prof. Kapur, for his patience and coping with me all these years.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.

